package ai

import (
	"encoding/gob"
	"fmt"
	"math"
	"math/rand"
	"os"
	"sync"
	"time"

	"github.com/kevin-chtw/tw_common/gamebase/mahjong"
	"github.com/topfreegames/pitaya/v3/pkg/logger"
	"gorgonia.org/gorgonia"
	"gorgonia.org/tensor"
)

var inst *RichAI
var once sync.Once
var globalLearnable bool // å…¨å±€è®­ç»ƒæ¨¡å¼æ ‡å¿—

type RichAI struct {
	inferNet      *DQNet // æ¨ç†ç½‘ç»œï¼ˆæ¸¸æˆç”¨ï¼Œå¿«é€Ÿï¼‰
	trainNet      *DQNet // è®­ç»ƒç½‘ç»œï¼ˆåå°è®­ç»ƒï¼Œç‹¬ç«‹ï¼‰
	per           *PER
	inferMu       sync.RWMutex // æ¨ç†ç½‘ç»œé”ï¼ˆä»…åœ¨åŒæ­¥æƒé‡æ—¶åŠ å†™é”ï¼‰
	trainMu       sync.Mutex   // è®­ç»ƒç½‘ç»œé”ï¼ˆè®­ç»ƒæ—¶ç‹¬å ï¼‰
	count         int
	trainStep     int                 // è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºå­¦ä¹ ç‡è¡°å‡
	epsilon       float32             // Îµ-greedy æ¢ç´¢ç‡
	trainQueue    chan *TrainingBatch // è®­ç»ƒé˜Ÿåˆ—ï¼ˆå­˜å‚¨4äººæ‰¹æ¬¡ï¼‰
	syncEvery     int                 // æ¯Næ­¥åŒæ­¥ä¸€æ¬¡æƒé‡
	batchBuffer   []*TrainingData     // æ‰¹é‡ç¼“å†²åŒºï¼Œæ”¶é›†4ä¸ªç©å®¶æ•°æ®
	batchBufferMu sync.Mutex          // ç¼“å†²åŒºé”
}

// SetTrainingMode è®¾ç½®å…¨å±€è®­ç»ƒæ¨¡å¼ï¼ˆåœ¨ç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨ï¼‰
func SetTrainingMode(enable bool) {
	globalLearnable = enable
}

// IsTrainingMode è¿”å›å½“å‰æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
func IsTrainingMode() bool {
	return globalLearnable
}

func GetRichAI() *RichAI {
	once.Do(func() {
		inferNet := NewDQNet()
		trainNet := NewDQNet()

		inst = &RichAI{
			inferNet:    inferNet,
			trainNet:    trainNet,
			per:         NewPER(20000),
			count:       0,
			trainStep:   0,
			epsilon:     0.2,                           // åˆå§‹æ¢ç´¢ç‡20%
			trainQueue:  make(chan *TrainingBatch, 25), // è®­ç»ƒé˜Ÿåˆ—ç¼“å†²25ä¸ªæ‰¹æ¬¡ï¼ˆæ¯æ‰¹4äººï¼‰
			syncEvery:   50,                            // æ¯50æ­¥åŒæ­¥ä¸€æ¬¡æƒé‡
			batchBuffer: make([]*TrainingData, 0, 4),   // é¢„åˆ†é…4ä¸ªç©ºé—´
		}

		// åŠ è½½æƒé‡åˆ°æ¨ç†ç½‘ç»œ
		loadWeights(inst.inferNet, "tw_mjsc_svr.gob")
		// åŒæ­¥æƒé‡åˆ°è®­ç»ƒç½‘ç»œ
		inst.syncWeights()

		// å¯åŠ¨å¤šä¸ªè®­ç»ƒåç¨‹ï¼ˆå¹¶å‘å¤„ç†ï¼‰
		if globalLearnable {
			go inst.trainWorker()
		}
	})
	return inst
}

func loadWeights(net *DQNet, path string) error {
	var w map[string][]float32
	f, err := os.Open(path)
	if err != nil {
		return err
	}
	defer f.Close()
	if err := gob.NewDecoder(f).Decode(&w); err != nil {
		return err
	}
	for _, n := range net.learnables {
		weightData, exists := w[n.Name()]
		if !exists {
			logger.Log.Warnf("Weight %s not found in file, using random initialization", n.Name())
			continue
		}
		expectedSize := 1
		for _, dim := range n.Shape() {
			expectedSize *= dim
		}
		if len(weightData) != expectedSize {
			logger.Log.Warnf("Weight %s shape mismatch. Expected %d elements, got %d. Using random initialization", n.Name(), expectedSize, len(weightData))
			continue
		}
		if err := gorgonia.Let(n, tensor.New(tensor.WithShape(n.Shape()...), tensor.WithBacking(weightData))); err != nil {
			logger.Log.Warnf("Failed to load weight %s: %v. Using random initialization", n.Name(), err)
			continue
		}
	}
	return nil
}

// Decision ç»Ÿä¸€å†³ç­–ç»“æœå’Œå†³ç­–è®°å½•
type Decision struct {
	Operate int          `json:"operate"`       // mahjong.Operate ç±»å‹
	Tile    mahjong.Tile `json:"tile"`          // ç‰Œå€¼
	QValue  float32      `json:"q_value"`       // å†³ç­–Qå€¼ï¼ˆç”¨äºé€‰æ‹©æœ€ä½³å†³ç­–ï¼‰
	Obs     []float32    `json:"obs,omitempty"` // æ“ä½œæ—¶çš„çŠ¶æ€ç‰¹å¾ï¼ˆç”¨äºè®­ç»ƒï¼Œå¯é€‰ï¼‰
}

// Step ç»Ÿä¸€å†³ç­–å…¥å£ - å¤–éƒ¨ä¼ å…¥å¯è¡Œæ“ä½œå’Œå±€é¢
func (ai *RichAI) Step(state *GameState) *Decision {

	// å…ˆç»Ÿä¸€è®¡ç®—ä¸€æ¬¡ Q å€¼ï¼Œé¿å…é‡å¤è®¡ç®—
	feat := state.ToRichFeature()
	obs := feat.ToVector()

	ai.inferMu.RLock()
	qValues := ai.inferNet.Forward(obs)
	epsilon := ai.epsilon
	ai.inferMu.RUnlock()

	var candidates []*Decision // æ”¶é›†æ‰€æœ‰å€™é€‰åŠ¨ä½œ
	state.SelfTurn = state.Operates.HasOperate(mahjong.OperateDiscard)

	if state.Operates.HasOperate(mahjong.OperateDiscard) {
		candidates = append(candidates, ai.addDiscards(state, qValues)...)
	}
	if state.Operates.HasOperate(mahjong.OperateHu) {
		if d := ai.hu(state, qValues); d != nil {
			candidates = append(candidates, d)
		}
	}
	if state.Operates.HasOperate(mahjong.OperatePon) {
		if d := ai.pon(state, qValues); d != nil {
			candidates = append(candidates, d)
		}
	}
	if state.Operates.HasOperate(mahjong.OperateKon) {
		candidates = append(candidates, ai.addKons(state, qValues)...)
	}
	if state.Operates.HasOperate(mahjong.OperatePass) {
		if d := ai.pass(state, qValues); d != nil {
			candidates = append(candidates, d)
		}
	}

	if len(candidates) == 0 {
		logger.Log.Errorf("==========================No candidates, epsilon=%.3f================", epsilon)
		return nil
	}

	// Îµ-greedy ç­–ç•¥ï¼šåœ¨æ‰€æœ‰å€™é€‰åŠ¨ä½œä¸­ç»Ÿä¸€åº”ç”¨
	var bestD *Decision
	if globalLearnable && len(candidates) > 1 && rand.Float32() < epsilon {
		// æ¢ç´¢ï¼šä»æ‰€æœ‰å€™é€‰åŠ¨ä½œä¸­éšæœºé€‰æ‹©
		bestD = candidates[rand.Intn(len(candidates))]
	} else {
		// åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
		for _, d := range candidates {
			if bestD == nil || d.QValue > bestD.QValue {
				bestD = d
			}
		}
	}

	if globalLearnable && bestD != nil && bestD.Operate != int(mahjong.OperateNone) {
		logger.Log.Infof("==========================RecordDecision  %v (epsilon=%.3f, candidates=%d)================", bestD, epsilon, len(candidates))
		state.RecordDecision(bestD.Operate, bestD.Tile)
	}

	return bestD
}

func (ai *RichAI) addTiles(state *GameState, qValues []float32, isLack bool) []*Decision {
	var result []*Decision
	lackColor := state.PlayerLacks[state.CurrentSeat]

	for tile, count := range state.Hand {
		if count <= 0 {
			continue
		}
		if isLack != (tile.Color() == lackColor) {
			continue
		}

		value := ai.evaluateState(qValues, mahjong.OperateDiscard, tile)
		result = append(result, &Decision{
			Operate: mahjong.OperateDiscard,
			Tile:    tile,
			QValue:  value,
		})
	}
	return result
}

func (ai *RichAI) addDiscards(state *GameState, qValues []float32) []*Decision {
	// ä¼˜å…ˆæ”¶é›†ç¼ºé—¨ç‰Œï¼Œå¦‚æœæ²¡æœ‰å†æ”¶é›†å…¶ä»–ç‰Œ
	result := ai.addTiles(state, qValues, true)
	if len(result) == 0 {
		result = ai.addTiles(state, qValues, false)
	}
	return result
}

func (ai *RichAI) hu(state *GameState, qValues []float32) *Decision {
	value := ai.evaluateState(qValues, mahjong.OperateHu, state.LastTile)
	return &Decision{
		Operate: mahjong.OperateHu,
		Tile:    state.LastTile,
		QValue:  value,
	}
}

func (ai *RichAI) pon(state *GameState, qValues []float32) *Decision {
	value := ai.evaluateState(qValues, mahjong.OperatePon, state.LastTile)
	return &Decision{
		Operate: mahjong.OperatePon,
		Tile:    state.LastTile,
		QValue:  value,
	}
}

func (ai *RichAI) addKons(state *GameState, qValues []float32) []*Decision {
	var result []*Decision
	tiles := make([]mahjong.Tile, 0)
	if state.SelfTurn {
		for _, t := range state.PonTiles[state.CurrentSeat] {
			if state.Hand[t] == 1 {
				tiles = append(tiles, t)
			}
		}
		for t, count := range state.Hand {
			if count == 4 {
				tiles = append(tiles, t)
			}
		}
	} else {
		tiles = append(tiles, state.LastTile)
	}

	for _, t := range tiles {
		value := ai.evaluateState(qValues, mahjong.OperateKon, t)
		result = append(result, &Decision{
			Operate: mahjong.OperateKon,
			Tile:    t,
			QValue:  value,
		})
	}
	return result
}

func (ai *RichAI) pass(_ *GameState, qValues []float32) *Decision {
	value := ai.evaluateState(qValues, mahjong.OperatePass, mahjong.TileNull)
	return &Decision{
		Operate: mahjong.OperatePass,
		Tile:    mahjong.TileNull,
		QValue:  value,
	}
}

func getActionIndex(operate int, tile mahjong.Tile) (int, error) {
	if operate == mahjong.OperatePass {
		return 136, nil
	}
	if !tile.IsValid() {
		return 0, fmt.Errorf("invalid tile: operate=%d, tile=%d", operate, tile)
	}
	tileIdx := mahjong.ToIndex(tile)
	actionIdx := actionIndex(operate, tileIdx)
	if actionIdx < 0 || actionIdx >= outputDim {
		return 0, fmt.Errorf("invalid action index: %d", actionIdx)
	}
	return actionIdx, nil
}

func (ai *RichAI) evaluateState(qValues []float32, operate int, tile mahjong.Tile) float32 {
	actionIdx, err := getActionIndex(operate, tile)
	if err != nil {
		logger.Log.Warnf("getActionIndex failed: %v, returning 0", err)
		return 0
	}

	return qValues[actionIdx]
}

// ä» obs ä¸­æå–å¯è¡ŒåŠ¨ä½œæ©ç å¹¶è®¡ç®—æ©ç åçš„æœ€å¤§ Q å€¼
// obs å‘é‡ä¸­ï¼ŒOperates çš„ 5 ç»´ one-hot ä½äºç´¢å¼• [600, 605)
// åŠ¨ä½œç´¢å¼•åŒºé—´å®šä¹‰è§ actionIndex:
//
//	Discard: [0,34), Pon: [34,68), Kon: [68,102), Hu: [102,136), Pass: [136,137)
func maxMaskedQFromObs(q []float32, obs []float32) float32 {
	const operatesOffset = 600
	const operatesDim = 5
	if len(obs) < operatesOffset+operatesDim {
		max := float32(-1e9)
		for _, v := range q {
			if v > max {
				max = v
			}
		}
		return max
	}
	ops := obs[operatesOffset : operatesOffset+operatesDim]
	ranges := make([][2]int, 0, 5)
	// Discard
	if ops[0] > 0.5 {
		ranges = append(ranges, [2]int{0, 34})
	}
	// Hu
	if ops[1] > 0.5 {
		ranges = append(ranges, [2]int{102, 136})
	}
	// Pon
	if ops[2] > 0.5 {
		ranges = append(ranges, [2]int{34, 68})
	}
	// Kon
	if ops[3] > 0.5 {
		ranges = append(ranges, [2]int{68, 102})
	}
	// Pass
	if ops[4] > 0.5 {
		ranges = append(ranges, [2]int{136, 137})
	}
	if len(ranges) == 0 {
		max := float32(-1e9)
		for _, v := range q {
			if v > max {
				max = v
			}
		}
		return max
	}
	max := float32(-1e9)
	for _, r := range ranges {
		for i := r[0]; i < r[1]; i++ {
			if q[i] > max {
				max = q[i]
			}
		}
	}
	return max
}

// TrainingData å‡†å¤‡å¥½çš„è®­ç»ƒæ•°æ®
type TrainingData struct {
	DecisionHistory []DecisionRecord
	ShapedReward    float32
	IsHu            bool
	HuMulti         int64
	IsLiuJu         bool
	DecisionSteps   int
}

// TrainingBatch 4ä¸ªç©å®¶çš„æ‰¹é‡è®­ç»ƒæ•°æ®
type TrainingBatch struct {
	Players []*TrainingData
}

// QueueTraining å°†è®­ç»ƒä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
func (ai *RichAI) QueueTraining(finalState *GameState) {
	if !globalLearnable {
		return
	}

	// ======== åœ¨æ¸¸æˆçº¿ç¨‹ä¸­è®¡ç®—å¥–åŠ±ï¼ˆä¸å ç”¨è®­ç»ƒçº¿ç¨‹ï¼‰ ========
	finalScore := finalState.FinalScore
	isLiuJu := finalState.IsLiuJu
	decisionSteps := len(finalState.DecisionHistory)

	// åˆ¤æ–­æ˜¯å¦èƒ¡ç‰ŒåŠç•ªæ•°
	isHu := false
	var huMulti int64
	for _, huSeat := range finalState.HuPlayers {
		if huSeat == finalState.CurrentSeat {
			isHu = true
			if multi, ok := finalState.HuMultis[huSeat]; ok {
				huMulti = multi
			}
			break
		}
	}

	// è®¡ç®—å¢å¼ºå¥–åŠ±ï¼ˆreward shapingï¼‰
	shapedReward := finalScore

	// 1. èƒ¡ç‰Œå¥–åŠ±ï¼šåŸºç¡€åˆ† + ç•ªæ•°åŠ æˆ
	if isHu {
		shapedReward += 10.0
		shapedReward += float32(huMulti) * 2.0
		if decisionSteps > 0 && decisionSteps < 50 {
			speedBonus := (50.0 - float32(decisionSteps)) / 10.0
			shapedReward += speedBonus
		}
	} else if isLiuJu {
		shapedReward -= 5.0
	}

	// æ„å»ºè®­ç»ƒæ•°æ®
	trainingData := &TrainingData{
		DecisionHistory: finalState.DecisionHistory,
		ShapedReward:    shapedReward,
		IsHu:            isHu,
		HuMulti:         huMulti,
		IsLiuJu:         isLiuJu,
		DecisionSteps:   decisionSteps,
	}

	logger.Log.Warnf("QueueTraining: isHu=%v, multi=%d, liuju=%v, steps=%d, shapedReward=%.2f",
		isHu, huMulti, isLiuJu, decisionSteps, shapedReward)

	// æ”¶é›†4ä¸ªç©å®¶çš„æ•°æ®åå†æäº¤è®­ç»ƒï¼ˆä¸€å±€æ¸¸æˆçš„å®Œæ•´æ•°æ®ï¼ŒåŒ…å«èƒœè€…å’Œè´¥è€…ï¼‰
	ai.batchBufferMu.Lock()
	ai.batchBuffer = append(ai.batchBuffer, trainingData)
	bufferLen := len(ai.batchBuffer)

	// å½“æ”¶é›†åˆ°4ä¸ªæ•°æ®æ—¶ï¼Œæ‰“åŒ…å‘é€
	if bufferLen >= 4 {
		// å¤åˆ¶ç¼“å†²åŒºæ•°æ®å¹¶æ„å»ºæ‰¹æ¬¡
		batch := &TrainingBatch{
			Players: make([]*TrainingData, len(ai.batchBuffer)),
		}
		copy(batch.Players, ai.batchBuffer)
		// æ¸…ç©ºç¼“å†²åŒº
		ai.batchBuffer = ai.batchBuffer[:0]
		ai.batchBufferMu.Unlock()

		logger.Log.Infof("ğŸ“¦ Collected 4-player batch, submitting to training queue")

		// éé˜»å¡å‘é€æ‰¹æ¬¡ï¼ˆä¸€æ¬¡æ€§å‘é€4ä¸ªç©å®¶æ•°æ®ï¼‰
		select {
		case ai.trainQueue <- batch:
		default:
			logger.Log.Warnf("Training queue full, dropping training batch")
		}
	} else {
		ai.batchBufferMu.Unlock()
	}
}

// trainWorker è®­ç»ƒå·¥ä½œåç¨‹ï¼ˆå•çº¿ç¨‹é¡ºåºå¤„ç†ï¼‰
func (ai *RichAI) trainWorker() {
	for batch := range ai.trainQueue {
		ai.trainning(batch)
	}
}

// trainning å¸¦æŒ‡æ ‡çš„è®­ç»ƒæ›´æ–°ï¼ˆæ”¯æŒ reward shapingï¼‰- å†…éƒ¨æ–¹æ³•
// æ¥æ”¶4ä¸ªç©å®¶çš„æ•°æ®æ‰¹æ¬¡ï¼Œä¸€èµ·è®­ç»ƒ
func (ai *RichAI) trainning(batch *TrainingBatch) {
	Î³ := float32(0.99)

	// ======== ä½¿ç”¨è®­ç»ƒç½‘ç»œè¿›è¡Œè®­ç»ƒï¼ˆä¸é˜»å¡æ¨ç†ï¼‰ ========
	startTime := time.Now()
	ai.trainMu.Lock()

	prepareStart := time.Now()

	// ç¬¬ä¸€æ­¥ï¼šæ”¶é›†4ä¸ªç©å®¶çš„æ‰€æœ‰å†³ç­–æ•°æ®åˆ°PER
	for _, data := range batch.Players {
		historyLen := len(data.DecisionHistory)
		nextMaxQ := float32(0.0)
		shapedReward := data.ShapedReward

		for i := historyLen - 1; i >= 0; i-- {
			decisionRec := &data.DecisionHistory[i]

			currQ := ai.trainNet.Forward(decisionRec.Obs)

			// å…ˆè®¡ç®—å½“å‰çŠ¶æ€çš„ max Q å€¼ï¼ˆå¸¦å¯è¡ŒåŠ¨ä½œæ©ç ï¼‰ï¼Œç”¨äºä¸‹ä¸€ä¸ªï¼ˆæ›´æ—©çš„ï¼‰å†³ç­–
			maxCurrQ := maxMaskedQFromObs(currQ, decisionRec.Obs)

			// è®¡ç®— TD ç›®æ ‡ï¼šæœ€åä¸€æ­¥ä½¿ç”¨ shapedRewardï¼Œå…¶ä»–æ­¥éª¤ä½¿ç”¨ Î³ * nextMaxQ
			var tdTarget float32
			if i == historyLen-1 {
				// æœ€åä¸€æ­¥ï¼šä½¿ç”¨æ¸©å’Œçš„å½’ä¸€åŒ–ï¼Œä¿ç•™æ›´å¤šä¿¡å·å¼ºåº¦
				// tanh(x/5) æ¯” tanh(x/20) ä¿ç•™æ›´å¤šå€¼çš„èŒƒå›´
				tdTarget = float32(math.Tanh(float64(shapedReward) / 5.0))
			} else {
				// å…¶ä»–æ­¥éª¤ï¼štdTarget = Î³ * nextMaxQï¼ˆä½¿ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ max Q å€¼ï¼‰
				tdTarget = Î³ * nextMaxQ
			}

			// æ›´æ–° nextMaxQ ä¸ºå½“å‰çŠ¶æ€çš„ max Q å€¼ï¼Œä¾›ä¸‹ä¸€ä¸ªï¼ˆæ›´æ—©çš„ï¼‰å†³ç­–ä½¿ç”¨
			nextMaxQ = maxCurrQ

			target := make([]float32, 137)
			copy(target, currQ)

			actionIdx, err := getActionIndex(decisionRec.Operate, decisionRec.Tile)
			if err != nil {
				logger.Log.Warnf("getActionIndex failed: %v, skipping", err)
				continue
			}

			// ç›´æ¥å°†ç›®æ ‡åŠ¨ä½œçš„ç›®æ ‡å€¼è®¾ç½®ä¸º tdTargetï¼ˆä¸è¿›è¡ŒäºŒæ¬¡"è½¯æ›´æ–°"ï¼‰
			target[actionIdx] = tdTarget
			tdErr := float32(math.Abs(float64(tdTarget - currQ[actionIdx])))

			ai.per.Add(decisionRec.Obs, target, float64(tdErr))
		}
	}
	prepareTime := time.Since(prepareStart)

	// ç¬¬ä¸‰æ­¥ï¼šä¸€æ¬¡æ€§è®­ç»ƒæ‰€æœ‰æ”¶é›†çš„æ ·æœ¬
	sampleStart := time.Now()
	states, targets := ai.per.Sample()
	sampleTime := time.Since(sampleStart)

	trainStart := time.Now()
	loss := ai.trainNet.Train(states, targets)
	trainTime := time.Since(trainStart)
	ai.trainStep++

	// epsilon è¡°å‡ï¼šä» 0.2 è¡°å‡åˆ° 0.05ï¼Œåœ¨ 10000 æ­¥åç¨³å®š
	if ai.trainStep < 10000 {
		ai.epsilon = 0.2 - (0.15 * float32(ai.trainStep) / 10000.0)
	} else {
		ai.epsilon = 0.05
	}
	currTrainStep := ai.trainStep
	currEpsilon := ai.epsilon
	batchSize := len(states)

	// å®šæœŸåŒæ­¥æƒé‡åˆ°æ¨ç†ç½‘ç»œ
	if currTrainStep%ai.syncEvery == 0 {
		ai.trainMu.Unlock() // å…ˆé‡Šæ”¾è®­ç»ƒé”
		ai.syncWeights()    // åŒæ­¥æƒé‡
		ai.trainMu.Lock()   // é‡æ–°è·å–è®­ç»ƒé”
		logger.Log.Warnf("âš¡ Synced weights from trainNet to inferNet at step %d", currTrainStep)
	}

	ai.trainMu.Unlock()

	totalTime := time.Since(startTime)

	// å¢å¼ºæ—¥å¿—ï¼šè®°å½•å…³é”®æŒ‡æ ‡å’Œæ€§èƒ½åˆ†æï¼ˆ4ç©å®¶æ‰¹æ¬¡ï¼‰
	huCount := 0
	for _, p := range batch.Players {
		if p.IsHu {
			huCount++
		}
	}
	logger.Log.Warnf("Train: loss=%.6f, trainStep=%d, epsilon=%.3f, batchSize=%d, 4-player-batch (huCount=%d)",
		loss, currTrainStep, currEpsilon, batchSize, huCount)
	logger.Log.Warnf("â±ï¸  Performance: total=%.3fs, prepare=%.3fs (%.1f%%), sample=%.3fs (%.1f%%), train=%.3fs (%.1f%%)",
		totalTime.Seconds(),
		prepareTime.Seconds(), prepareTime.Seconds()/totalTime.Seconds()*100,
		sampleTime.Seconds(), sampleTime.Seconds()/totalTime.Seconds()*100,
		trainTime.Seconds(), trainTime.Seconds()/totalTime.Seconds()*100)
}

// syncWeights å°†è®­ç»ƒç½‘ç»œçš„æƒé‡åŒæ­¥åˆ°æ¨ç†ç½‘ç»œ
func (ai *RichAI) syncWeights() {
	ai.trainMu.Lock()
	weights := make(map[string][]float32)
	for _, n := range ai.trainNet.learnables {
		data := n.Value().Data().([]float32)
		// æ·±æ‹·è´æƒé‡
		weightCopy := make([]float32, len(data))
		copy(weightCopy, data)
		weights[n.Name()] = weightCopy
	}
	ai.trainMu.Unlock()

	// æ›´æ–°æ¨ç†ç½‘ç»œï¼ˆéœ€è¦å†™é”ï¼‰
	ai.inferMu.Lock()
	for _, n := range ai.inferNet.learnables {
		if w, ok := weights[n.Name()]; ok {
			// ç›´æ¥æ›¿æ¢åº•å±‚æ•°æ®
			data := n.Value().Data().([]float32)
			copy(data, w)
		}
	}
	ai.inferMu.Unlock()
}

// SaveWeights æŠŠç½‘ç»œæƒé‡è½ç›˜åˆ°æ–‡ä»¶
func (ai *RichAI) SaveWeights(path string) error {
	if !globalLearnable {
		return nil // éè®­ç»ƒæ¨¡å¼ä¸ä¿å­˜æƒé‡
	}
	if ai.trainStep%40 != 0 {
		return nil
	}
	logger.Log.Infof("==================================SaveWeights================")
	ai.trainMu.Lock()
	defer ai.trainMu.Unlock()
	w := make(map[string][]float32) // å¯¼å‡ºå­—æ®µ
	for _, n := range ai.trainNet.learnables {
		w[n.Name()] = n.Value().Data().([]float32)
	}
	f, err := os.Create(path)
	if err != nil {
		logger.Log.Errorf("==================================SaveWeights failed: %v", err)
		return err
	}
	defer f.Close()
	return gob.NewEncoder(f).Encode(w)
}
