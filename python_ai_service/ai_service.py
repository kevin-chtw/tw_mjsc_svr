#!/usr/bin/env python3
"""
éº»å°†AIè®­ç»ƒæœåŠ¡ - ä½¿ç”¨HTTP REST API
æ”¯æŒDQNè®­ç»ƒï¼ˆå¯é€‰PyTorchï¼Œæ— GPUä¹Ÿèƒ½è¿è¡Œï¼‰
"""

from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import numpy as np
import random
from collections import deque
from datetime import datetime
import sys
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('ai_service.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    HAS_TORCH = True
    logger.info("âœ… PyTorch available")
except ImportError:
    HAS_TORCH = False
    logger.warning("âš ï¸  PyTorch not available, using random policy")

class DQN(nn.Module if HAS_TORCH else object):
    """Dueling DQN ç½‘ç»œ - é€‚åˆéº»å°†AI
    
    ç‰¹ç‚¹ï¼š
    1. Duelingæ¶æ„ï¼šåˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’ŒåŠ¨ä½œä¼˜åŠ¿A(s,a)
    2. æ®‹å·®è¿æ¥ï¼šæé«˜æ·±å±‚ç½‘ç»œçš„è®­ç»ƒæ•ˆæœ
    3. LayerNormï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
    4. åˆç†çš„ç½‘ç»œæ·±åº¦ï¼šè¶³å¤Ÿè¡¨è¾¾å¤æ‚ç­–ç•¥ï¼Œä½†ä¸ä¼šè¿‡æ‹Ÿåˆ
    """
    def __init__(self, input_dim=3185, hidden_dim=512, output_dim=137):
        if HAS_TORCH:
            super().__init__()
            
            # å…±äº«ç‰¹å¾æå–å±‚ï¼ˆæ®‹å·®å—ï¼‰
            self.shared = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
            )
            
            # æ®‹å·®å—1
            self.residual1 = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
            )
            
            # æ®‹å·®å—2
            self.residual2 = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
            )
            
            # Duelingæ¶æ„ï¼šçŠ¶æ€ä»·å€¼æµ
            self.value_stream = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, 1)
            )
            
            # Duelingæ¶æ„ï¼šåŠ¨ä½œä¼˜åŠ¿æµ
            self.advantage_stream = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, output_dim)
            )
        
    def forward(self, x):
        if not HAS_TORCH:
            return None
            
        # å…±äº«ç‰¹å¾æå–
        shared_features = self.shared(x)
        
        # æ®‹å·®è¿æ¥1
        residual = self.residual1(shared_features)
        shared_features = torch.relu(shared_features + residual)
        
        # æ®‹å·®è¿æ¥2
        residual = self.residual2(shared_features)
        shared_features = torch.relu(shared_features + residual)
        
        # Duelingæ¶æ„ï¼šè®¡ç®—V(s)å’ŒA(s,a)
        value = self.value_stream(shared_features)
        advantages = self.advantage_stream(shared_features)
        
        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        # å‡å»å¹³å‡å€¼æ˜¯ä¸ºäº†å”¯ä¸€æ€§ï¼ˆidentifiabilityï¼‰
        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))
        
        return q_values

class AIService:
    def __init__(self):
        self.device = "cpu"
        if HAS_TORCH:
            self.model = DQN()
            self.target_model = DQN()
            self.target_model.load_state_dict(self.model.state_dict())
            # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼ˆå¸¦æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            # é™ä½æƒé‡è¡°å‡ï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–
            self.optimizer = optim.AdamW(self.model.parameters(), lr=0.0003, weight_decay=0.001)
            # ä½¿ç”¨Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
            self.criterion = nn.SmoothL1Loss()
            # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ›´æ…¢çš„è¡°å‡ï¼Œä¿æŒå­¦ä¹ èƒ½åŠ›ï¼‰
            # æ¯2000æ­¥è¡°å‡ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯1000æ­¥
            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=2000, gamma=0.95)
        else:
            self.model = None
            self.target_model = None
        
        # æ›´å¤§çš„replay bufferï¼Œå­˜å‚¨æ›´å¤šç»éªŒ
        self.replay_buffer = deque(maxlen=50000)
        # epsilonå‚æ•°
        self.epsilon = 1.0  # ä»é«˜æ¢ç´¢å¼€å§‹
        self.epsilon_min = 0.15  # æé«˜æœ€å°å€¼ï¼Œä¿æŒæ›´å¤šæ¢ç´¢ï¼ˆä»0.1æå‡åˆ°0.15ï¼‰
        self.epsilon_decay = 0.9995  # æ›´ç¼“æ…¢çš„è¡°å‡
        # è®­ç»ƒå‚æ•°
        self.gamma = 0.99
        self.batch_size = 128  # æ›´å¤§çš„batch sizeï¼Œè®­ç»ƒæ›´ç¨³å®š
        self.train_count = 0
        self.update_target_every = 100  # æ›´é¢‘ç¹æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.save_every = 1000  # æ¯1000æ­¥è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡æ¨¡å‹
        self.last_save_count = 0  # ä¸Šæ¬¡ä¿å­˜çš„è®­ç»ƒæ­¥æ•°
        # ä¼˜å…ˆç»éªŒå›æ”¾å‚æ•°ï¼ˆå¯é€‰ï¼‰
        self.use_prioritized_replay = False  # æš‚æ—¶å…³é—­ï¼Œç®€åŒ–å®ç°
        
        logger.info(f"ğŸš€ AI Service initialized (PyTorch: {HAS_TORCH})")
        logger.info(f"   Model params: {sum(p.numel() for p in self.model.parameters()) if HAS_TORCH else 0:,}")
    
    def get_decision(self, obs, candidates):
        """ä»å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©æœ€ä½³åŠ¨ä½œ"""
        if not candidates:
            return {'operate': 1, 'tile': 0}  # OPERATE_PASS
        
        # æ£€æŸ¥PASSå€™é€‰çš„tileå€¼
        for cand in candidates:
            if cand['operate'] == 1 and cand['tile'] != 0:
                logger.warning(f"âš ï¸  Received PASS candidate with tile={cand['tile']}, should be 0!")
        
        # ä½¿ç”¨DQNé€‰æ‹©æœ€ä½³åŠ¨ä½œ
        if HAS_TORCH and random.random() > self.epsilon:
            # è®¡ç®—æ‰€æœ‰å€™é€‰åŠ¨ä½œçš„Qå€¼
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            with torch.no_grad():
                q_values = self.model(obs_tensor).squeeze(0).numpy()
            
            best_candidate = None
            best_q = float('-inf')
            
            for cand in candidates:
                action_idx = self._get_action_index(cand['operate'], cand['tile'])
                if action_idx is not None and action_idx < len(q_values):
                    q = q_values[action_idx]
                    if q > best_q:
                        best_q = q
                        best_candidate = cand
            
            if best_candidate:
                return self._normalize_decision(best_candidate)
        
        # éšæœºé€‰æ‹©æˆ–DQNå¤±è´¥æ—¶çš„fallback
        selected = random.choice(candidates)
        return self._normalize_decision(selected)
    
    def _normalize_decision(self, decision):
        """æ ‡å‡†åŒ–å†³ç­–ï¼šPASSæ“ä½œçš„tileç»Ÿä¸€ä¸º0"""
        OPERATE_PASS = 1
        original_tile = decision['tile']
        if decision['operate'] == OPERATE_PASS:
            if original_tile != 0:
                logger.warning(f"âš ï¸  Normalizing PASS decision: tile {original_tile} -> 0")
            return {'operate': OPERATE_PASS, 'tile': 0}
        return decision    
  
    def _get_action_index(self, operate, tile):
        """è®¡ç®—åŠ¨ä½œç´¢å¼•ï¼ˆ0-136ï¼‰
        æ³¨æ„ï¼šGoä¾§å·²ç»å°†tileè½¬æ¢ä¸ºindex(0-33)å†ä¼ è¿‡æ¥
        """
        OPERATE_PASS = 1
        OPERATE_PON = 4
        OPERATE_KON = 8
        OPERATE_HU = 32
        OPERATE_DISCARD = 64
        
        # tileå·²ç»æ˜¯index(0-33)ï¼Œç›´æ¥ä½¿ç”¨
        if operate == OPERATE_DISCARD:
            return tile
        elif operate == OPERATE_PON:
            return 34 + tile
        elif operate == OPERATE_KON:
            return 68 + tile
        elif operate == OPERATE_HU:
            return 102 + tile
        elif operate == OPERATE_PASS:
            return 136
        return None
    
    def get_action(self, state, valid_actions):
        """æ¨ç†ï¼šæ ¹æ®çŠ¶æ€å’Œæœ‰æ•ˆåŠ¨ä½œè¿”å›æœ€ä½³åŠ¨ä½œï¼ˆæ—§æ¥å£ï¼‰"""
        if not valid_actions:
            return 0
        
        # Îµ-greedy
        if random.random() < self.epsilon or not HAS_TORCH:
            return random.choice(valid_actions)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.model(state_tensor).squeeze(0).numpy()
            
            # åªè€ƒè™‘æœ‰æ•ˆåŠ¨ä½œ
            valid_q = [(idx, q_values[idx]) for idx in valid_actions]
            return max(valid_q, key=lambda x: x[1])[0]
    
    def report_episode(self, episode_data):
        """è®­ç»ƒï¼šæ¥æ”¶ä¸€å±€è½¨è¿¹"""
        steps = episode_data.get('steps', [])
        
        # å°†è½¨è¿¹åŠ å…¥ replay buffer
        for step in steps:
            state = np.array(step['state'], dtype=np.float32)
            next_state = np.array(step.get('next_state', []), dtype=np.float32) if step.get('next_state') else None
            
            # ä»operateå’Œtileè®¡ç®—action_idx
            action_idx = self._get_action_index(step['operate'], step['tile'])
            if action_idx is None:
                continue  # è·³è¿‡æ— æ•ˆåŠ¨ä½œ
            
            self.replay_buffer.append({
                'state': state,
                'action': action_idx,
                'reward': step['reward'],
                'next_state': next_state,
                'done': step.get('done', False)
            })
        
        # å¦‚æœbufferè¶³å¤Ÿå¤§ä¸”æœ‰PyTorchï¼Œè¿›è¡Œè®­ç»ƒ
        if HAS_TORCH and len(self.replay_buffer) >= self.batch_size:
            loss = self._train()
            
            # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
            if self.train_count % self.update_target_every == 0:
                self.target_model.load_state_dict(self.model.state_dict())
                logger.info(f"ğŸ”„ Updated target network at train step {self.train_count}")
        
        is_hu = episode_data.get('is_hu', False)
        hu_multi = episode_data.get('hu_multi', 0)
        shaped_reward = episode_data.get('shaped_reward', 0)
        
        logger.info(f"ğŸ“¦ Episode: {len(steps)} steps, buffer: {len(self.replay_buffer)}, hu: {is_hu}, multi: {hu_multi}, reward: {shaped_reward:.2f}")
        return {'status': 'ok'}
    
    def _train(self):
        """ä» replay buffer é‡‡æ ·å¹¶è®­ç»ƒ"""
        if not HAS_TORCH or len(self.replay_buffer) < self.batch_size:
            return 0.0
        
        batch = random.sample(self.replay_buffer, self.batch_size)
        
        # ä½¿ç”¨ numpy.stack æé«˜è½¬æ¢æ•ˆç‡
        states_np = np.stack([t['state'] for t in batch])
        actions_np = np.array([t['action'] for t in batch], dtype=np.int64)
        rewards_np = np.array([t['reward'] for t in batch], dtype=np.float32)
        next_states_np = np.stack([
            t['next_state'] if t['next_state'] is not None else np.zeros_like(t['state']) 
            for t in batch
        ])
        dones_np = np.array([t['done'] for t in batch], dtype=np.float32)
        
        # è½¬æ¢ä¸º tensorï¼ˆæ›´å¿«ï¼‰
        states = torch.from_numpy(states_np)
        actions = torch.from_numpy(actions_np)
        rewards = torch.from_numpy(rewards_np)
        next_states = torch.from_numpy(next_states_np)
        dones = torch.from_numpy(dones_np)
        
        # è®¡ç®—å½“å‰ Q å€¼
        current_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # æ­£ç¡®çš„ Double DQN å®ç°ï¼š
        # 1. å…ˆç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
        # 2. å†ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
        # è¿™æ ·å¯ä»¥é¿å…Qå€¼é«˜ä¼°
        with torch.no_grad():
            next_actions = self.model(next_states).max(1)[1].unsqueeze(1)
            next_q = self.target_model(next_states).gather(1, next_actions).squeeze(1)
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # è®¡ç®—æŸå¤±å¹¶æ›´æ–°
        loss = self.criterion(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        # é™ä½max_normï¼Œä½¿æ¢¯åº¦æ›´æ–°æ›´æ•æ„Ÿ
        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # ç›‘æ§æ¢¯åº¦ï¼ˆæ¯100æ¬¡è®­ç»ƒè®°å½•ä¸€æ¬¡ï¼‰
        if self.train_count % 100 == 0 and grad_norm < 0.001:
            logger.warning(f"âš ï¸  Gradient too small: {grad_norm:.6f}, model may not be learning!")
        
        # epsilon è¡°å‡ï¼ˆæŒ‡æ•°è¡°å‡åˆ°æœ€å°å€¼ï¼‰
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler.step()
        
        self.train_count += 1
        
        # å®šæœŸæ‰“å°è®­ç»ƒä¿¡æ¯
        if self.train_count % 20 == 0:
            current_lr = self.optimizer.param_groups[0]['lr']
            # è®¡ç®—å¹³å‡Qå€¼ï¼Œç›‘æ§æ¨¡å‹è¾“å‡º
            avg_q = current_q.mean().item()
            avg_target_q = target_q.mean().item()
            logger.info(f"ğŸ”¥ Train #{self.train_count}: loss={loss.item():.6f}, epsilon={self.epsilon:.3f}, lr={current_lr:.6f}, buffer={len(self.replay_buffer)}, avg_q={avg_q:.3f}, avg_target_q={avg_target_q:.3f}")
        
        # è‡ªåŠ¨ä¿å­˜æ¨¡å‹
        if self.train_count - self.last_save_count >= self.save_every:
            self.save_model()
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
            backup_path = f'mahjong_dqn_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pth'
            self.save_model(backup_path)
            self.last_save_count = self.train_count
            logger.info(f"ğŸ’¾ Auto-saved model (train_count={self.train_count})")
        
        return loss.item()
    
    def save_model(self, path='mahjong_dqn.pth'):
        """ä¿å­˜æ¨¡å‹"""
        if HAS_TORCH:
            current_lr = self.optimizer.param_groups[0]['lr']
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'target_model_state_dict': self.target_model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'epsilon': self.epsilon,
                'train_count': self.train_count,
                'buffer_size': len(self.replay_buffer),
                'learning_rate': current_lr,  # ä¿å­˜å½“å‰å­¦ä¹ ç‡
            }, path)
            logger.info(f"ğŸ’¾ Model saved to {path} (train_count={self.train_count}, epsilon={self.epsilon:.3f}, lr={current_lr:.8f})")
    
    def load_model(self, path='mahjong_dqn.pth', reset_lr=True, reset_epsilon=True):
        """åŠ è½½æ¨¡å‹
        
        Args:
            path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            reset_lr: æ˜¯å¦é‡ç½®å­¦ä¹ ç‡ï¼ˆé»˜è®¤Trueï¼Œé‡ç½®åˆ°0.0002ï¼‰
            reset_epsilon: æ˜¯å¦é‡ç½®æ¢ç´¢ç‡ï¼ˆé»˜è®¤Trueï¼Œé‡ç½®åˆ°0.2ï¼‰
        """
        if HAS_TORCH:
            try:
                checkpoint = torch.load(path, weights_only=True)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                
                # é‡ç½®å­¦ä¹ ç‡ï¼ˆå¦‚æœå­¦ä¹ ç‡è¿‡ä½ï¼‰
                if reset_lr:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    if current_lr < 0.0001:  # å¦‚æœå­¦ä¹ ç‡è¿‡ä½ï¼Œé‡ç½®
                        new_lr = 0.0002  # é‡ç½®åˆ°0.0002
                        for param_group in self.optimizer.param_groups:
                            param_group['lr'] = new_lr
                        logger.info(f"ğŸ”„ Learning rate reset: {current_lr:.8f} -> {new_lr:.6f}")
                    else:
                        logger.info(f"âœ… Learning rate OK: {current_lr:.6f}")
                
                # é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½¿ç”¨æ–°çš„å­¦ä¹ ç‡ï¼‰
                if reset_lr:
                    self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=2000, gamma=0.95)
                    logger.info(f"ğŸ”„ Learning rate scheduler reset")
                elif 'scheduler_state_dict' in checkpoint:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                
                # é‡ç½®æ¢ç´¢ç‡
                if reset_epsilon:
                    self.epsilon = 0.2  # é‡ç½®åˆ°0.2ï¼Œä¿æŒä¸€å®šæ¢ç´¢
                    logger.info(f"ğŸ”„ Epsilon reset to: {self.epsilon:.3f}")
                else:
                    self.epsilon = checkpoint.get('epsilon', 0.2)
                    # ç¡®ä¿epsilonä¸ä½äºæœ€å°å€¼
                    self.epsilon = max(self.epsilon_min, self.epsilon)
                
                self.train_count = checkpoint.get('train_count', 0)
                self.last_save_count = self.train_count  # æ¢å¤ä¸Šæ¬¡ä¿å­˜çš„æ­¥æ•°
                buffer_size = checkpoint.get('buffer_size', 0)
                logger.info(f"ğŸ“‚ Model loaded from {path}")
                logger.info(f"   Train count: {self.train_count}, Epsilon: {self.epsilon:.3f}, Buffer was: {buffer_size}")
            except FileNotFoundError:
                logger.warning(f"âš ï¸  Model file not found: {path}, starting fresh")
            except Exception as e:
                logger.warning(f"âš ï¸  Error loading model: {e}, starting fresh")

# å…¨å±€æœåŠ¡å®ä¾‹
ai_service = AIService()
# å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
ai_service.load_model()

class RequestHandler(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        # å‡å°‘æ—¥å¿—è¾“å‡º
        pass
    
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b'{}'
        
        try:
            data = json.loads(post_data.decode('utf-8')) if post_data else {}
            
            if self.path == '/get_action':
                # GetAction æ¥å£
                state = data['state']
                valid_actions = data['valid_actions']
                action_idx = ai_service.get_action(state, valid_actions)
                
                response = {'action_idx': int(action_idx)}
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(response).encode())
                
            elif self.path == '/get_decision':
                # GetDecision æ¥å£ - ä»å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©æœ€ä½³åŠ¨ä½œ
                obs = data['obs']
                candidates = data['candidates']  # [{operate, tile}, ...]
                
                decision = ai_service.get_decision(obs, candidates)
                
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(decision).encode())
            
            elif self.path == '/report_episode':
                # ReportEpisode æ¥å£
                result = ai_service.report_episode(data)
                
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(result).encode())
                
            elif self.path == '/save_model':
                # ä¿å­˜æ¨¡å‹æ¥å£
                ai_service.save_model()
                
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps({'status': 'saved'}).encode())
            else:
                self.send_error(404)
        
        except Exception as e:
            logger.error(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            self.send_error(500, str(e))
    
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({
                'status': 'healthy',
                'pytorch': HAS_TORCH,
                'buffer_size': len(ai_service.replay_buffer),
                'epsilon': ai_service.epsilon,
                'train_count': ai_service.train_count
            }).encode())
        else:
            self.send_error(404)

def serve(port=50051):
    server = HTTPServer(('0.0.0.0', port), RequestHandler)
    logger.info(f"âœ… AI Service listening on http://0.0.0.0:{port}")
    logger.info(f"   GET  /health - å¥åº·æ£€æŸ¥")
    logger.info(f"   POST /get_decision - è·å–å†³ç­–")
    logger.info(f"   POST /report_episode - ä¸ŠæŠ¥è½¨è¿¹")
    logger.info(f"   POST /save_model - ä¿å­˜æ¨¡å‹")
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ Shutting down...")
        # é€€å‡ºå‰ä¿å­˜æ¨¡å‹
        ai_service.save_model()
        server.shutdown()

if __name__ == '__main__':
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 50051
    serve(port)

